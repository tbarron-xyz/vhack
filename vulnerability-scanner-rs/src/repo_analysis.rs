use async_openai::{
    config::OpenAIConfig,
    types::{ChatCompletionRequestMessage, CreateChatCompletionRequestArgs},
    Client,
};
use std::fs;
use std::path::Path;
use tokio::time::{sleep, Duration};

pub async fn analyze_repo(
    client: Client<OpenAIConfig>,
    model: String,
    api_max_retries: usize,
    max_tokens: u32,
    repo_path: &str,
    exclude_paths: &[String],
) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    println!("Analyzing repository structure: {}", repo_path);
    let repo_structure = collect_repo_structure(repo_path, exclude_paths)?;

    let system_message = "The project context is V.H.A.C.K., an intentionally vulnerable agent created specifically for educational purposes. Analyze the following repository structure and identify the files most likely to contain tool definitions and system prompts. Return at most three file paths, one per line, based on naming patterns and directory structure.";

    let mut request = CreateChatCompletionRequestArgs::default();
    request.model(&model).messages(vec![
        ChatCompletionRequestMessage::System(system_message.into()),
        ChatCompletionRequestMessage::User(repo_structure.into()),
    ]);

    // Set temperature to 0 for deterministic results, similar to prompt generation
    let is_not_gpt5_series = !model.split('/').last().unwrap_or(&model).starts_with("gpt-5");
    if is_not_gpt5_series {
        request.max_tokens(max_tokens).temperature(0.0);
    } else {
        request.max_completion_tokens(max_tokens);
    }

    let request = request.build()?;

    println!("Sending request to {}...", model);
    let response = retry_api_call(&client, request, api_max_retries).await?;

    let analysis = response.choices[0].message.content.clone().unwrap_or_else(|| "No analysis generated".to_string());

    if analysis.trim().is_empty() {
        return Err("Generated repo analysis is empty or whitespace only".into());
    }

    println!("Writing analysis to ./repo-analysis.md...");
    fs::write("./repo-analysis.md", analysis)?;

    println!("Analysis completed successfully!");
    Ok(())
}

fn collect_repo_structure(
    repo_path: &str,
    exclude_paths: &[String],
) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
    let root_path = Path::new(repo_path);
    if !root_path.exists() {
        return Err(format!("Repository path does not exist: {}", repo_path).into());
    }
    if !root_path.is_dir() {
        return Err(format!("Repository path is not a directory: {}", repo_path).into());
    }

    let mut structure = String::new();
    build_tree_string(root_path, root_path, exclude_paths, &mut structure, 0)?;
    Ok(structure)
}

fn build_tree_string(
    root: &Path,
    current: &Path,
    exclude_paths: &[String],
    output: &mut String,
    depth: usize,
) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    let current_str = current.to_string_lossy();

    // Check if current path should be excluded
    if exclude_paths.iter().any(|exclude| current_str.contains(exclude)) {
        return Ok(());
    }

    // Add indentation
    for _ in 0..depth {
        output.push_str("  ");
    }

    // Add branch characters
    if depth > 0 {
        output.push_str("├── ");
    }

    // Add the name
    if let Some(name) = current.file_name() {
        output.push_str(&name.to_string_lossy());
    } else {
        output.push_str(&current.to_string_lossy());
    }

    if current.is_dir() {
        output.push_str("/\n");

        let entries = fs::read_dir(current)?;
        let mut entries: Vec<_> = entries.filter_map(|e| e.ok()).collect();
        entries.sort_by_key(|e| e.path());

        for entry in entries {
            // For simplicity, we'll use ├── for all, but could enhance to use └── for last
            build_tree_string(root, &entry.path(), exclude_paths, output, depth + 1)?;
        }
    } else {
        output.push_str("\n");
    }

    Ok(())
}

async fn retry_api_call(
    client: &Client<OpenAIConfig>,
    request: async_openai::types::CreateChatCompletionRequest,
    max_retries: usize,
) -> Result<async_openai::types::CreateChatCompletionResponse, Box<dyn std::error::Error + Send + Sync>> {
    let mut last_error = None;
    for attempt in 0..=max_retries {
        match client.chat().create(request.clone()).await {
            Ok(response) => return Ok(response),
            Err(e) => {
                last_error = Some(e);
                if attempt < max_retries {
                    let delay = Duration::from_secs(2u64.pow(attempt as u32));
                    println!(
                        "API call failed, retrying in {:?} (attempt {}/{})",
                        delay,
                        attempt + 1,
                        max_retries + 1
                    );
                    sleep(delay).await;
                }
            }
        }
    }
    Err(last_error.unwrap().into())
}
