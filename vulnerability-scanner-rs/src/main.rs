use async_openai::{config::OpenAIConfig, Client};
use std::env;
use std::time::{Duration, Instant};
use tokio::time::timeout;

mod cli;
mod prompt_generator;
mod prompt_processing;
mod repo_analysis;
mod result_classification;
mod results_report;
mod tool_analysis;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    let cli_args = cli::parse_args();

    let (api_key, base_url) = cli::get_api_credentials(&cli_args.provider)?;
    let openai_client = Client::with_config(OpenAIConfig::new().with_api_key(api_key).with_api_base(base_url));

    // Handle prompt generation mode
    if cli_args.generate_prompts {
        let mut generator = prompt_generator::PromptGenerator::new(
            openai_client,
            cli_args.model,
            cli_args.api_max_retries,
            cli_args.max_tokens,
        );
        return generator.generate_prompts(&cli_args.prompts_file, cli_args.concurrency).await;
    }

    // Handle analyze tools mode
    if cli_args.analyze_tools {
        return tool_analysis::analyze_tools(
            openai_client,
            cli_args.model,
            cli_args.api_max_retries,
            cli_args.max_tokens,
            &cli_args.tools_file,
        )
        .await;
    }

    // Handle analyze repo mode
    if cli_args.analyze_repo {
        return repo_analysis::analyze_repo(
            openai_client,
            cli_args.model,
            cli_args.api_max_retries,
            cli_args.max_tokens,
            &cli_args.repo_path,
            &cli_args.exclude_paths,
        )
        .await;
    }

    let all_args: Vec<String> = env::args().collect();

    let start_time = Instant::now();

    if let Some(ms) = cli_args.timeout_ms {
        match timeout(
            Duration::from_millis(ms),
            prompt_processing::run_processing(
                openai_client.clone(),
                cli_args.filter_test_type.clone(),
                cli_args.target_url,
                cli_args.prompts_file,
                cli_args.concurrency,
                cli_args.api_max_retries,
                cli_args.evaluation_model.clone(),
                cli_args.max_tokens,
            ),
        )
        .await
        {
            Ok(Ok((results, elapsed))) => {
                results_report::generate_summary_report(
                    &results,
                    cli_args.filter_test_type.as_ref(),
                    elapsed,
                    &all_args,
                    &cli_args.output_file,
                )?;
                Ok(())
            }
            Ok(Err(e)) => Err(e),
            Err(_) => {
                eprintln!("Overall timeout of {}ms reached", ms);
                let elapsed = start_time.elapsed();
                let results = results_report::read_results_from_file("results.temp.jsonl")?;
                results_report::generate_summary_report(
                    &results,
                    cli_args.filter_test_type.as_ref(),
                    elapsed,
                    &all_args,
                    &cli_args.output_file,
                )?;
                Ok(())
            }
        }
    } else {
        let (results, elapsed) = prompt_processing::run_processing(
            openai_client,
            cli_args.filter_test_type.clone(),
            cli_args.target_url,
            cli_args.prompts_file,
            cli_args.concurrency,
            cli_args.api_max_retries,
            cli_args.evaluation_model.clone(),
            cli_args.max_tokens,
        )
        .await?;
        results_report::generate_summary_report(
            &results,
            cli_args.filter_test_type.as_ref(),
            elapsed,
            &all_args,
            &cli_args.output_file,
        )?;
        Ok(())
    }
}
